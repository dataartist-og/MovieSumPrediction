# -*- coding: utf-8 -*-
"""whitespace_shallowml.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pvh9pjVb-uq4XwagBfevEalY0PBZMuq1
"""







# Commented out IPython magic to ensure Python compatibility.
# %%shell
# #wget https://bit.ly/3lH1hKU 
# pip install bert-tensorflow

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2



# Commented out IPython magic to ensure Python compatibility.
# %%shell
# #cp /content/drive/MyDrive/kaggle.json /root/.kaggle/
# #kaggle datasets download -d rounakbanik/the-movies-dataset
# #unzip the-movies-dataset.zip

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# #unzip /content/MovieSummaries.tar.gz
# #7z x  /content/MovieSummaries.tar.gz
# #tar -xvzf  /content/MovieSummaries.tar.gz
# pip3 install transformers

import transformers

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import json
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import nltk
import joblib

# %matplotlib inline

#data = pd.read_csv("movies_metadata.csv", sep = ',', header = 0, low_memory=False)
#movies = data[['title','overview','genres']].copy()
#movies.head()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import json
import nltk
import re
import csv
import matplotlib.pyplot as plt 
import seaborn as sns
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# %matplotlib inline

file_path = '/content/drive/MyDrive/MovieSummaries/'

meta = pd.read_csv(file_path+"movie.metadata.tsv", sep = '\t')
meta.head()

# rename columns
meta.columns = ["movie_id",1,"movie_name",3,4,5,6,7,"genre"]

meta.head()

import csv
from tqdm.notebook import tqdm

plots = []

with open(file_path+"plot_summaries.txt", 'r') as f:
       reader = csv.reader(f, dialect='excel-tab') 
       for row in tqdm(reader):
            if len(row) < 2: continue
            plots.append(row)





plots_df = pd.DataFrame(plots)

plots_df.columns = ['movie_id','plot']

# change datatype of 'movie_id'
meta['movie_id'] = meta['movie_id'].astype(str)

# merge meta with movies
movies = pd.merge(plots_df, meta[['movie_id', 'movie_name', 'genre']], on = 'movie_id')

movies.head()

gs = movies.genre.apply(json.loads)
gs = gs.apply(lambda s: s.values())



movies.genre = gs

# remove samples with 0 genre tags
movies_new = movies[movies.genre.apply(lambda s: len(s)!=0)].copy()
#movies_new.genre = movies_new.genre.apply(lambda s: list(s.values()))

movies_new.shape, movies.shape

movies_new.genre.values[0]

from collections import Counter
sorted(Counter(all_genres).items(),key=lambda c: c[0])

clusts = ( ('Animal Picture',  
 'Animals'),
 ('Anti-war', 
 'Anti-war film'), 
 ('Biographical film', 
 'Biography', 
 'Biopic [feature]'), 
 ('Comedy', 
 'Comedy film'),
 ('Coming of age', 
 'Coming-of-age film'), 
 ('Education', 
 'Educational'), 
 ('Filipino', 
 'Filipino Movies'), 
 ('Gay', 
 'Gay Interest', 
 'Gay Themed'), 
 ('Gross out', 
 'Gross-out film'), 
 ('Monster', 
 'Monster movie'), 
 ('Pornographic movie', 
 'Pornography'), 
 ('Prison', 
 'Prison film'), 
 ('Sci Fi Pictures original films', 
 'Science Fiction'), 
 ('Social issues', 
 'Social problem film'), 
 ('Superhero', 
 'Superhero movie'), 
 ('Sword and sorcery', 
 'Sword and sorcery films'), 
 ('Tamil cinema', 
 'Tollywood') )

for cl in tqdm(clusts):
  movies_new.genre = movies_new.genre.apply(lambda ss: [cl[0] if s in cl else s for s in ss])

# get all genre tags in a list
all_genres = []
movies_new.genre.apply(all_genres.extend)
len(set(all_genres))

all_genres = nltk.FreqDist(all_genres) 

# create dataframe
all_genres_df = pd.DataFrame({'Genre': list(all_genres.keys()), 
                              'Count': list(all_genres.values())})

g = all_genres_df.nlargest(columns="Count", n = 50) 
plt.figure(figsize=(12,15)) 
ax = sns.barplot(data=g, x= "Count", y = "Genre") 
ax.set(ylabel = 'Count') 
plt.show()

nltk.download('stopwords')





movies_new.genre.apply(len).plot(kind='hist')

train = movies_new[['plot','genre']].dropna()

from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()

from pickle import dump,load
try:
  mlb = load(open(file_path+'label_binarizer.pkl', 'rb'))
except:
  mlb = mlb.fit(train.genre.values)

train.head()

#train.genre = train.genre.apply(lambda l: list(l)[0])
train.head()

train['og_genres'] = train.genre

train.genre = train.genre.apply(lambda s: mlb.transform([s]))

train.genre =train.genre.apply(lambda s: s.flatten())



# function for text cleaning 
def clean_text(text):
    # remove backslash-apostrophe 
    text = re.sub("'", "", text) 
    # remove everything except alphabets 
    text = re.sub("[^a-zA-Z]"," ",text) 
    # remove whitespaces 
    text = ' '.join(text.split()) 
    # convert text to lowercase 
    text = text.lower() 
    
    return text

train['clean_plot'] = train['plot'].apply(lambda x: clean_text(x))

def freq_words(x, terms = 30): 
  all_words = ' '.join([text for text in x]) 
  all_words = all_words.split() 
  fdist = nltk.FreqDist(all_words) 
  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())}) 
  
  # selecting top 20 most frequent words 
  d = words_df.nlargest(columns="count", n = terms) 
  
  # visualize words and frequencies
  plt.figure(figsize=(12,15)) 
  ax = sns.barplot(data=d, x= "count", y = "word") 
  ax.set(ylabel = 'Word') 
  plt.show()
  
# print 100 most frequent words 
freq_words(train['clean_plot'], 100)

nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

# function to remove stopwords
def remove_stopwords(text):
    no_stopword_text = [w for w in text.split() if not w in stop_words]
    return ' '.join(no_stopword_text)

train['clean_plot'] = train['clean_plot'].apply(lambda x: remove_stopwords(x))

freq_words(train['clean_plot'], 100)

y = np.vstack(train['genre'].values)

tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)

y

# split dataset into training and validation set
xtrain, xval, ytrain, yval = train_test_split(train['clean_plot'], y, test_size=0.26, random_state=9)
xtest, xval, ytest, yval = train_test_split(xval, yval, test_size=0.25, random_state=9)



vectorizer =TfidfVectorizer(min_df=10,ngram_range=(1,3), max_features=20000)
xtrain_multilabel1 = vectorizer.fit_transform(xtrain)
xval_multilabel1 = vectorizer.transform(xval)
xtest_multilabel1 = vectorizer.transform(xtest)

print("Dimensions of train data X:",xtrain_multilabel1.shape, "Y :",ytrain.shape)
print("Dimensions of val data X:",xval_multilabel1.shape, "Y :",yval.shape)
print("Dimensions of test data X:",xtest_multilabel1.shape,"Y:",ytest.shape)

from sklearn.decomposition import LatentDirichletAllocation
lda=LatentDirichletAllocation()
tr2=lda.fit_transform(xtrain_multilabel1)
test2=lda.transform(xtest_multilabel1)

from datetime import datetime



(((ytrain.mean(axis=0)-1))==0).sum(),ytrain.mean(axis=0).shape



from sklearn.multiclass import *#OneVsRestClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import  *
start = datetime.now()
alpha=[0.001,0.01,0.1,1,10]
for i in alpha:
    classifier_1 = OneVsRestClassifier(BernoulliNB(alpha=i))
    classifier_1.fit(xtrain_multilabel1, ytrain)
    predictions_1 = classifier_1.predict(xtrain_multilabel1)
    predictions_2 = classifier_1.predict(xval_multilabel1)
    f1 = f1_score(ytrain, predictions_1, average='micro')
    f2 = f1_score(yval, predictions_2, average='micro')
    print("Micro-average quality numbers for C=",i)
    print(" F1-measure for train: {:.4f}".format( f1))
    print(" F1-measure for val: {:.4f}".format( f2))
print("Time taken to run this cell :", datetime.now() - start)

import sklearn

start = datetime.now()
alpha=[0.01,0.1,1,10]
for j in alpha:
    classifier_1 = OneVsRestClassifier(sklearn.linear_model.LogisticRegression(C=j,solver='saga',penalty='l1',tol=0.001))
    classifier_1.fit(xtrain_multilabel1, ytrain)
    ypred_prob = classifier_1.predict_proba(xtrain_multilabel1)
    i=0.2
    predictions_1 = (ypred_prob >= i).astype(int)
    ypred_probc = classifier_1.predict_proba(xval_multilabel1)
 
    predictions_2 = (ypred_probc >= i).astype(int)
    
    f1 = f1_score(ytrain, predictions_1, average='micro')
    f2 = f1_score(yval, predictions_2, average='micro')
    print("Micro-average quality numbers for C=",j)
    print(" F1-measure for train: {:.4f}".format( f1))
    print(" F1-measure for val: {:.4f}".format( f2))
print("Time taken to run this cell :", datetime.now() - start)

# I am keeping same hyperparameter which i got after tuning above
classifier_1 = OneVsRestClassifier(sklearn.linear_model.LogisticRegression(C=1,penalty='l2',tol=0.001))
classifier_1.fit(xtrain_multilabel1, ytrain)
yprobtr= classifier_1.predict_proba(xtrain_multilabel1)
ypred_prob = classifier_1.predict_proba(xtest_multilabel1 )
i=0.2 
ypredtr = (yprobtr >= i).astype(int)
ypred_new = (ypred_prob >= i).astype(int)
f1 = f1_score(ytrain,ypredtr, average='micro')
f2 = f1_score(ytest, ypred_new, average='micro')
print("Micro-average quality numbers for C=1 and threshold=",i)
print(" F1-measure for test: {:.4f}".format( f1))
print(" F1-measure for test: {:.4f}".format( f2))

from tensorflow.keras.layers import *
import tensorflow as tf



embedding_vecor_length = 300
MAX_SEQ_LENGTH = 512
model_1 = tf.keras.Sequential()
model_1.add(Embedding(13780+1, embedding_vecor_length, input_length=MAX_SEQ_LENGTH))
model_1.add(LSTM(128))
model_1.add(Dense(len(set(all_genres)), activation='sigmoid'))

loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)

model_1.compile(loss=loss, optimizer='adam', metrics=['accuracy'])



model_1.fit(xtrain_multilabel1.toarray(), ytrain, epochs=5, batch_size=512,validation_data=(xval_multilabel1.toarray(),yval))

